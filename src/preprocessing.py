import re
import pandas as pd
import unicodedata
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

# =======================================================
# ‚öôÔ∏è 1. H√ÄM CHU·∫®N HO√Å TEXT C∆† B·∫¢N (Core Normalization)
# =======================================================
def normalize_text(text: str) -> str:
    """
    Chu·∫©n ho√° text: lowercase, remove accents (Unicode), remove special chars.
    ƒê√¢y l√† b∆∞·ªõc c·∫ßn thi·∫øt cho c·∫£ TF-IDF v√† SBERT.
    """
    if not isinstance(text, str):
        return ""
        
    # 1. Lowercase
    text = text.lower()
    
    # 2. Remove accents (X√≥a d·∫•u ti·∫øng Vi·ªát, Ti·∫øng Ph√°p, v.v. - D√πng unicodedata)
    # R·∫•t quan tr·ªçng ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n c·ªßa t·ª´ kh√≥a
    text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode("utf-8", "ignore")
    
    # 3. Remove special chars (Ch·ªâ gi·ªØ l·∫°i ch·ªØ c√°i v√† s·ªë)
    # D√πng regex ƒë·ªÉ thay th·∫ø m·ªçi th·ª© kh√¥ng ph·∫£i ch·ªØ c√°i, s·ªë, ho·∫∑c kho·∫£ng tr·∫Øng b·∫±ng kho·∫£ng tr·∫Øng
    text = re.sub(r"[^a-zA-Z0-9 ]+", " ", text)
    
    # 4. Remove extra spaces
    text = re.sub(r"\s+", " ", text).strip()
    
    return text


# =======================================================
# ‚öôÔ∏è 2. H√ÄM LO·∫†I STOPWORDS (D√†nh ri√™ng cho TF-IDF)
# =======================================================
def remove_stopwords(text: str) -> str:
    """
    Lo·∫°i b·ªè c√°c t·ª´ v√¥ nghƒ©a (stopwords) kh·ªèi chu·ªói vƒÉn b·∫£n.
    """
    if not isinstance(text, str):
        return ""
        
    words = text.split()
    # S·ª≠ d·ª•ng b·ªô stopwords chu·∫©n c·ªßa Scikit-learn
    filtered = [w for w in words if w not in ENGLISH_STOP_WORDS]
    return " ".join(filtered)


# =======================================================
# ‚öôÔ∏è 3. H√ÄM H·ªñ TR·ª¢: G·ªòP C√ÅC TR∆Ø·ªúNG D·ªÆ LI·ªÜU AN TO√ÄN
# =======================================================
def _build_text_core(row, field_list):
    """
    G·ªôp c√°c tr∆∞·ªùng d·ªØ li·ªáu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh th√†nh m·ªôt chu·ªói duy nh·∫•t,
    ngƒÉn c√°ch b·∫±ng d·∫•u ch·∫•m ƒë·ªÉ duy tr√¨ t√≠nh ng·ªØ nghƒ©a.
    """
    parts = []
    
    # L·∫∑p qua danh s√°ch c√°c c·ªôt c·∫ßn g·ªôp (V√≠ d·ª•: ['title', 'genre', 'review'])
    for field in field_list:
        # S·ª≠ d·ª•ng .get() an to√†n v√† ƒë·∫£m b·∫£o gi√° tr·ªã l√† chu·ªói
        value = row.get(field)
        if pd.notna(value) and str(value).strip():
            parts.append(str(value))
    
    return ". ".join(parts)


# =======================================================
# üîπ 4. T·∫†O TEXT ƒê·∫¶U V√ÄO CHO TF-IDF (similarity_text)
# =======================================================
def build_similarity_text(row):
    """
    T·∫°o text cho TF-IDF: G·ªôp c√°c tr∆∞·ªùng, chu·∫©n h√≥a, v√† lo·∫°i b·ªè stopwords.
    Text ƒë∆∞·ª£c l√†m s·∫°ch gi√∫p TF-IDF t·∫≠p trung v√†o t·ª´ kh√≥a.
    """
    # G·ªôp t·∫•t c·∫£ c√°c tr∆∞·ªùng quan tr·ªçng cho n·ªôi dung v√† ng·ªØ c·∫£nh
    field_list = ['title', 'genre', 'description', 'review', 'director', 'cast']
    
    joined_text = _build_text_core(row, field_list)
    
    # 1. Chu·∫©n h√≥a (Normalize)
    cleaned = normalize_text(joined_text)
    
    # 2. Lo·∫°i b·ªè Stopwords
    cleaned = remove_stopwords(cleaned)
    
    return cleaned


# =======================================================
# üîπ 5. T·∫†O TEXT ƒê·∫¶U V√ÄO CHO SBERT (full_text)
# =======================================================
def build_full_text(row):
    """
    T·∫°o text cho SBERT: G·ªôp t·∫•t c·∫£ c√°c tr∆∞·ªùng, ch·ªâ chu·∫©n h√≥a (gi·ªØ l·∫°i stopwords).
    SBERT c·∫ßn ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß.
    """
    # G·ªôp t·∫•t c·∫£ c√°c tr∆∞·ªùng (Gi·ªØ nguy√™n field_list nh∆∞ tr√™n)
    field_list = ['title', 'genre', 'description', 'review', 'director', 'cast']
    
    joined_text = _build_text_core(row, field_list)
    
    # Ch·ªâ Chu·∫©n h√≥a (Normalize)
    cleaned = normalize_text(joined_text)
    
    return cleaned


# =======================================================
# üîπ 6. TI·ªÄN X·ª¨ L√ù C√ÇU TRUY V·∫§N (D√πng cho Semantic Search)
# =======================================================
def preprocess_query(query: str, model_type: str = 'sbert') -> str:
    """
    Ti·ªÅn x·ª≠ l√Ω c√¢u truy v·∫•n c·ªßa ng∆∞·ªùi d√πng.
    query: C√¢u h·ªèi ng∆∞·ªùi d√πng nh·∫≠p v√†o.
    model_type: 'tfidf' ho·∫∑c 'sbert'.
    """
    # 1. Chu·∫©n h√≥a c∆° b·∫£n
    query = normalize_text(query)

    if model_type == 'tfidf':
        # 2. Lo·∫°i b·ªè stopwords (Quan tr·ªçng ƒë·ªÉ kh·ªõp t·ª´ kh√≥a v·ªõi ma tr·∫≠n TF-IDF ƒë√£ l√†m s·∫°ch)
        query = remove_stopwords(query)
    
    # ƒê·ªëi v·ªõi SBERT: ch·ªâ c·∫ßn normalize_text (ƒë√£ th·ª±c hi·ªán ·ªü b∆∞·ªõc 1)
    
    return query


# =======================================================
# üîπ 7. H√ÄM CH√çNH: Load cleaned dataset + t·∫°o c√°c c·ªôt x·ª≠ l√Ω
# =======================================================
def load_and_process_data(path: str = "../data/clean_movies.csv") -> pd.DataFrame:
    """
    Load d·ªØ li·ªáu s·∫°ch, t·∫°o c√°c c·ªôt text ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω cho 2 m√¥ h√¨nh.
    """
    try:
        df = pd.read_csv(path)
    except FileNotFoundError:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file t·∫°i ƒë∆∞·ªùng d·∫´n {path}. H√£y ki·ªÉm tra l·∫°i.")
        return pd.DataFrame()
    
    print("--- B·∫Øt ƒë·∫ßu t·∫°o c·ªôt vƒÉn b·∫£n ƒë·∫ßu v√†o cho m√¥ h√¨nh ---")
    
    # 1. T·∫°o c·ªôt cho TF-IDF (similarity_text)
    df["similarity_text"] = df.apply(build_similarity_text, axis=1)

    # 2. T·∫°o c·ªôt cho SBERT (full_text)
    df["full_text"] = df.apply(build_full_text, axis=1)

    print("--- Ho√†n t·∫•t t·∫°o vƒÉn b·∫£n x·ª≠ l√Ω ---")
    return df