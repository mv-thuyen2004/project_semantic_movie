# src/recommender_sbert.py
"""
ğŸ¬ Há»‡ thá»‘ng Gá»£i Ã½ Phim dá»±a trÃªn SBERT (Sentence-BERT)
Semantic Search vÃ  Content-Based Recommendation sá»­ dá»¥ng embeddings

LÆ¯U Ã: File nÃ y lÃ  THÆ¯ VIá»†N, chá»‰ Ä‘á»ƒ import vÃ o app.py
"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import torch
import sys
import os

# ThÃªm path Ä‘á»ƒ import tá»« src
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.preprocessing import preprocess_query

class SBERTRecommender:
    """
    Lá»›p gá»£i Ã½ phim sá»­ dá»¥ng SBERT embeddings (Memory Safe - Dynamic Calculation)
    """
    
    def __init__(self, sbert_model, sbert_embeddings, df):
        """
        Khá»Ÿi táº¡o recommender vá»›i SBERT model vÃ  embeddings
        
        Args:
            sbert_model: SBERT model Ä‘Ã£ loaded
            sbert_embeddings: Embeddings cá»§a toÃ n bá»™ dataset (tensor hoáº·c numpy array)
            df: DataFrame chá»©a thÃ´ng tin phim
        """
        self.sbert_model = sbert_model
        self.sbert_embeddings = sbert_embeddings
        self.df = df
        
        # Táº¡o mapping Ä‘á»ƒ tra cá»©u nhanh title -> index
        self.movie_indices_map = pd.Series(df.index, index=df['title']).drop_duplicates()
        
        # Chuyá»ƒn embeddings sang numpy náº¿u lÃ  tensor
        if torch.is_tensor(self.sbert_embeddings):
            self.sbert_embeddings = self.sbert_embeddings.cpu().numpy()
        
        print("âœ… SBERT Recommender khá»Ÿi táº¡o thÃ nh cÃ´ng")
        print(f"ğŸ“Š Embeddings shape: {self.sbert_embeddings.shape}")
    
    def search_movies(self, query, top_k=10, similarity_threshold=0.3):
        """
        TÃ¬m kiáº¿m phim dá»±a trÃªn query text sá»­ dá»¥ng SBERT embeddings
        
        Args:
            query (str): CÃ¢u truy váº¥n cá»§a ngÆ°á»i dÃ¹ng
            top_k (int): Sá»‘ káº¿t quáº£ tráº£ vá»
            similarity_threshold (float): NgÆ°á»¡ng similarity tá»‘i thiá»ƒu
            
        Returns:
            list: Danh sÃ¡ch dictionary chá»©a thÃ´ng tin phim
        """
        # Preprocess query (dÃ¹ng model_type='sbert' - giá»¯ nguyÃªn ngá»¯ nghÄ©a)
        processed_query = preprocess_query(query, model_type='sbert')
        
        if not processed_query.strip():
            return []
        
        try:
            # TÃ­nh embedding cho query
            query_embedding = self.sbert_model.encode(
                [processed_query],
                convert_to_tensor=True,
                normalize_embeddings=True,
                show_progress_bar=False
            )
            
            # Chuyá»ƒn sang numpy Ä‘á»ƒ tÃ­nh similarity
            query_embedding_np = query_embedding.cpu().numpy()
            
        except Exception as e:
            print(f"Lá»—i encode query: {e}")
            return []
        
        # TÃ­nh cosine similarity 1Ã—N
        similarities = cosine_similarity(query_embedding_np, self.sbert_embeddings).flatten()
        
        # Láº¥y top K indices
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # Táº¡o káº¿t quáº£
        results = []
        for idx in top_indices:
            similarity_score = similarities[idx]
            
            # SBERT thÆ°á»ng cÃ³ similarity scores tháº¥p hÆ¡n TF-IDF
            if similarity_score > similarity_threshold:
                results.append({
                    'title': self.df.iloc[idx]['title'],
                    'genre': self.df.iloc[idx]['genre'],
                    'description': self.df.iloc[idx].get('description', ''),
                    'year': self.df.iloc[idx].get('year', 'N/A'),
                    'poster': self.df.iloc[idx].get('poster', ''),
                    'similarity_score': float(similarity_score),
                    'original_index': int(idx)
                })
        
        return results
    
    def get_similar_movies(self, movie_title, top_k=10, exclude_self=True):
        """
        Gá»£i Ã½ phim tÆ°Æ¡ng tá»± dá»±a trÃªn SBERT embeddings
        
        Args:
            movie_title (str): TÃªn phim cáº§n tÃ¬m phim tÆ°Æ¡ng tá»±
            top_k (int): Sá»‘ phim tÆ°Æ¡ng tá»± tráº£ vá»
            exclude_self (bool): CÃ³ loáº¡i bá» phim gá»‘c khá»i káº¿t quáº£ khÃ´ng
            
        Returns:
            list: Danh sÃ¡ch phim tÆ°Æ¡ng tá»±
        """
        # TÃ¬m index cá»§a phim
        if movie_title not in self.movie_indices_map:
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y phim: {movie_title}")
            return []
        
        movie_idx = self.movie_indices_map[movie_title]
        
        # Láº¥y embedding cá»§a phim gá»‘c vÃ  tÃ­nh similarity 1Ã—N
        movie_embedding = self.sbert_embeddings[movie_idx].reshape(1, -1)
        similarities = cosine_similarity(movie_embedding, self.sbert_embeddings).flatten()
        
        # Sáº¯p xáº¿p vÃ  láº¥y top K
        sorted_indices = np.argsort(similarities)[::-1]
        
        # Loáº¡i bá» phim gá»‘c náº¿u cáº§n
        if exclude_self:
            sorted_indices = sorted_indices[sorted_indices != movie_idx]
        
        top_indices = sorted_indices[:top_k]
        
        # Táº¡o káº¿t quáº£
        similar_movies = []
        for idx in top_indices:
            similar_movies.append({
                'title': self.df.iloc[idx]['title'],
                'genre': self.df.iloc[idx]['genre'],
                'description': self.df.iloc[idx].get('description', ''),
                'year': self.df.iloc[idx].get('year', 'N/A'),
                'poster': self.df.iloc[idx].get('poster', ''),
                'similarity_score': float(similarities[idx]),
                'original_index': int(idx)
            })
        
        return similar_movies
    
    def hybrid_search(self, query, top_k=10):
        """
        TÃ¬m kiáº¿m lai giá»¯a semantic search vÃ  content-based recommendation
        
        Args:
            query (str): CÃ¢u truy váº¥n
            top_k (int): Sá»‘ káº¿t quáº£ tráº£ vá»
            
        Returns:
            list: Danh sÃ¡ch káº¿t quáº£ káº¿t há»£p
        """
        # TÃ¬m kiáº¿m semantic vá»›i SBERT
        semantic_results = self.search_movies(query, top_k=top_k//2)
        
        # Náº¿u cÃ³ káº¿t quáº£, láº¥y phim Ä‘áº§u tiÃªn vÃ  gá»£i Ã½ phim tÆ°Æ¡ng tá»±
        if semantic_results:
            best_match_idx = semantic_results[0]['original_index']
            similar_movies = self.get_similar_movies_by_index(best_match_idx, top_k=top_k//2)
            
            # Káº¿t há»£p káº¿t quáº£ (loáº¡i bá» trÃ¹ng láº·p)
            combined_results = semantic_results + similar_movies
            
            # Loáº¡i bá» trÃ¹ng láº·p dá»±a trÃªn title
            seen_titles = set()
            unique_results = []
            
            for movie in combined_results:
                if movie['title'] not in seen_titles:
                    seen_titles.add(movie['title'])
                    unique_results.append(movie)
            
            return unique_results[:top_k]
        else:
            return self.search_movies(query, top_k=top_k)


# HÃ m tiá»‡n Ã­ch Ä‘á»ƒ load model - DÃ™NG TRONG APP.PY
def load_sbert_models(model_path, embeddings_path, data_path):
    """
    Load SBERT models vÃ  embeddings tá»« file
    
    Args:
        model_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n SBERT model
        embeddings_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n embeddings
        data_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n dá»¯ liá»‡u phim
        
    Returns:
        SBERTRecommender: Instance cá»§a recommender
    """
    # TÃªn mÃ´ hÃ¬nh fallback
    FALLBACK_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
    sbert_model = None
    
    try:
        # 1. KIá»‚M TRA VÃ€ XÃC Äá»ŠNH THIáº¾T Bá»Š
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”„ Using device: {device}")
        
        # 2. THá»¬ Táº¢I MODEL Tá»ª ÄÆ¯á»œNG DáºªN Cá»¤C Bá»˜
        print("ğŸ”„ Äang thá»­ load SBERT model tá»« local...")
        sbert_model = SentenceTransformer(model_path, device=device)
        print("âœ… Load model local thÃ nh cÃ´ng.")
        
    except Exception as e:
        # 3. Náº¾U Lá»–I, Táº¢I Dá»° PHÃ’NG Tá»ª HUGGING FACE
        print(f"âš ï¸ Lá»—i táº£i model local: {e}")
        print(f"ğŸ”„ Äang táº£i dá»± phÃ²ng tá»« Hugging Face...")
        try:
            sbert_model = SentenceTransformer(FALLBACK_MODEL_NAME, device=device)
            print("âœ… Táº£i fallback model tá»« HF thÃ nh cÃ´ng.")
        except Exception as e_hf:
            print(f"âŒ Lá»—i: KhÃ´ng thá»ƒ táº£i fallback model tá»« HF: {e_hf}")
            return None

    # 4. Táº¢I EMBEDDINGS VÃ€ DATA
    try:
        print("ğŸ”„ Äang load embeddings...")
        if embeddings_path.endswith('.pt'):
            sbert_embeddings = torch.load(embeddings_path, map_location=device)
        else:
            sbert_embeddings = np.load(embeddings_path)
        
        print("ğŸ”„ Äang load data...")
        df = pd.read_csv(data_path)
        
        print("âœ… ÄÃ£ load SBERT models thÃ nh cÃ´ng")
        return SBERTRecommender(sbert_model, sbert_embeddings, df)
        
    except Exception as e_final:
        print(f"âŒ Lá»—i khi load embeddings/data: {e_final}")
        return None


def load_sbert_models_from_huggingface(model_name="sentence-transformers/all-MiniLM-L6-v2", embeddings_path=None, data_path=None):
    """
    Load SBERT model tá»« HuggingFace vÃ  embeddings tá»« file
    
    Args:
        model_name (str): TÃªn model trÃªn HuggingFace
        embeddings_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n embeddings
        data_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n dá»¯ liá»‡u phim
        
    Returns:
        SBERTRecommender: Instance cá»§a recommender
    """
    try:
        # XÃ¡c Ä‘á»‹nh device
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”„ Using device: {device}")
        
        # Load SBERT model tá»« HuggingFace
        print(f"ğŸ”„ Äang load SBERT model: {model_name}")
        sbert_model = SentenceTransformer(model_name, device=device)
        
        # Load embeddings
        if embeddings_path and os.path.exists(embeddings_path):
            print("ğŸ”„ Äang load embeddings...")
            if embeddings_path.endswith('.pt'):
                sbert_embeddings = torch.load(embeddings_path, map_location=device)
            else:
                sbert_embeddings = np.load(embeddings_path)
        else:
            raise FileNotFoundError(f"Embeddings file khÃ´ng tá»“n táº¡i: {embeddings_path}")
        
        # Load data
        df = pd.read_csv(data_path)
        
        print("âœ… ÄÃ£ load SBERT models thÃ nh cÃ´ng")
        return SBERTRecommender(sbert_model, sbert_embeddings, df)
        
    except Exception as e:
        print(f"âŒ Lá»—i khi load SBERT models: {e}")
        return None


# KHÃ”NG CÃ“ PHáº¦N DEMO/MAIN - FILE CHá»ˆ Äá»‚ IMPORT